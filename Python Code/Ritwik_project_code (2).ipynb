{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb2a7f0-77b9-432b-abf3-93d749c8f33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "#!pip install mlxtend pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aa5a0-de17-40cd-8f79-5108830b228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available databases:\n",
      "1. Amazon\n",
      "2. BestBuy\n",
      "3. KMart\n",
      "4. Nike\n",
      "5. Generic\n",
      "0. Exit\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import time\n",
    "\n",
    "# Define file paths for the datasets (use raw strings to avoid escape sequence issues)\n",
    "file_paths = {\n",
    "    \"Amazon\": r\"C:\\Users\\rithv\\Downloads\\Amazon_Updated_Transactions.csv\",\n",
    "    \"BestBuy\": r\"C:\\Users\\rithv\\Downloads\\BestBuy_Updated_Transactions.csv\",\n",
    "    \"KMart\": r\"C:\\Users\\rithv\\Downloads\\KMart_Updated_Transactions.csv\",\n",
    "    \"Nike\": r\"C:\\Users\\rithv\\Downloads\\Nike_Updated_Transactions.csv\",\n",
    "    \"Generic\" : r\"C:\\Users\\rithv\\midterm_project\\Generic_Updated_Transactions.csv\"\n",
    "}\n",
    "\n",
    "# Load transactions from CSV files\n",
    "def load_transactions(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    transactions = df['Items'].apply(lambda x: x.split(', ')).tolist()\n",
    "    return transactions\n",
    "\n",
    "# Brute Force Method for generating frequent itemsets\n",
    "def generate_frequent_itemsets(transactions, support_threshold):\n",
    "    item_count = {}\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_count[item] = item_count.get(item, 0) + 1\n",
    "\n",
    "    frequent_itemsets = {1: {item: count for item, count in item_count.items() if count / len(transactions) >= support_threshold}}\n",
    "\n",
    "    k = 2\n",
    "    while True:\n",
    "        prev_itemsets = list(frequent_itemsets[k - 1].keys())\n",
    "        new_itemsets = list(combinations(prev_itemsets, k))\n",
    "        item_count = {}\n",
    "        for transaction in transactions:\n",
    "            transaction_set = set(transaction)\n",
    "            for itemset in new_itemsets:\n",
    "                if set(itemset).issubset(transaction_set):\n",
    "                    item_count[itemset] = item_count.get(itemset, 0) + 1\n",
    "\n",
    "        frequent_itemsets[k] = {itemset: count for itemset, count in item_count.items() if count / len(transactions) >= support_threshold}\n",
    "        if not frequent_itemsets[k]:\n",
    "            del frequent_itemsets[k]\n",
    "            break\n",
    "        k += 1\n",
    "    return frequent_itemsets\n",
    "\n",
    "# Apriori Algorithm\n",
    "def apriori_algorithm(transactions, support_threshold, confidence_threshold):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = apriori(df, min_support=support_threshold, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_threshold)\n",
    "\n",
    "    return frequent_itemsets, rules\n",
    "\n",
    "# FP-Growth Algorithm\n",
    "def fpgrowth_algorithm(transactions, support_threshold, confidence_threshold):\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "    frequent_itemsets = fpgrowth(df, min_support=support_threshold, use_colnames=True)\n",
    "    rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=confidence_threshold)\n",
    "\n",
    "    return frequent_itemsets, rules\n",
    "\n",
    "# Timing function for comparison\n",
    "def measure_execution_time(algorithm_func, *args):\n",
    "    start_time = time.time()\n",
    "    result = algorithm_func(*args)\n",
    "    end_time = time.time()\n",
    "    return result, end_time - start_time\n",
    "\n",
    "# Main program loop\n",
    "while True:\n",
    "    # Prompt user to select a database or exit\n",
    "    print(\"\\nAvailable databases:\")\n",
    "    for i, name in enumerate(file_paths.keys(), 1):\n",
    "        print(f\"{i}. {name}\")\n",
    "    print(\"0. Exit\")\n",
    "    \n",
    "    choice = int(input(\"Enter the number corresponding to the database you'd like to choose (or 0 to exit): \"))\n",
    "\n",
    "    # Exit the loop if the user chooses 0\n",
    "    if choice == 0:\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n",
    "\n",
    "    # Get the selected database name\n",
    "    db_name = list(file_paths.keys())[choice - 1]\n",
    "\n",
    "    # Load the selected transactions\n",
    "    transactions = load_transactions(file_paths[db_name])\n",
    "    print(f\"Loaded {len(transactions)} transactions from {db_name}.\")\n",
    "\n",
    "    # Prompt user for support and confidence thresholds\n",
    "    support_threshold = float(input(\"Enter support threshold in % (e.g., 10 for 10%): \")) / 100\n",
    "    confidence_threshold = float(input(\"Enter confidence threshold in % (e.g., 20 for 20%): \")) / 100\n",
    "\n",
    "    print(f\"\\nProcessing {db_name} with support {support_threshold * 100}% and confidence {confidence_threshold * 100}%...\")\n",
    "\n",
    "    # Brute Force\n",
    "    bf_result, bf_time = measure_execution_time(generate_frequent_itemsets, transactions, support_threshold)\n",
    "    print(f\"\\nBrute Force Frequent Itemsets:\\n{bf_result}\")\n",
    "    print(f\"Brute Force Time: {bf_time:.4f}s\")\n",
    "\n",
    "    # Apriori\n",
    "    apriori_result, apriori_time = measure_execution_time(apriori_algorithm, transactions, support_threshold, confidence_threshold)\n",
    "    print(f\"\\nApriori Frequent Itemsets:\\n{apriori_result[0]}\")\n",
    "    print(f\"Apriori Rules:\\n{apriori_result[1]}\")\n",
    "    print(f\"Apriori Time: {apriori_time:.4f}s\")\n",
    "\n",
    "    # FP-Growth\n",
    "    fp_result, fp_time = measure_execution_time(fpgrowth_algorithm, transactions, support_threshold, confidence_threshold)\n",
    "    print(f\"\\nFP-Growth Frequent Itemsets:\\n{fp_result[0]}\")\n",
    "    print(f\"FP-Growth Rules:\\n{fp_result[1]}\")\n",
    "    print(f\"FP-Growth Time: {fp_time:.4f}s\")\n",
    "\n",
    "    # Performance summary\n",
    "    print(\"\\nTiming Performance Comparison \")\n",
    "    print(f\"Brute Force Time: {bf_time:.4f}s\")\n",
    "    print(f\"Apriori Time: {apriori_time:.4f}s\")\n",
    "    print(f\"FP-Growth Time: {fp_time:.4f}s\")\n",
    "\n",
    "    # Ask if the user wants to run another analysis\n",
    "    continue_choice = input(\"\\nDo you want to analyze another dataset? (yes/no): \").strip().lower()\n",
    "    if continue_choice != 'yes':\n",
    "        print(\"Exiting the program.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33accfa-eeaa-4a6e-b1c9-7e1711a363a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
